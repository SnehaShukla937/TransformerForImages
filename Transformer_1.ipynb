{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Transformer_1.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn"
      ],
      "metadata": {
        "id": "YFF8qSh2CSZC"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **1. Patch Embedding:**\n",
        "\n",
        "Split images into patches and then embed them."
      ],
      "metadata": {
        "id": "AbHiF9r7BSPz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "WVB0Zp4Zxrb3"
      },
      "outputs": [],
      "source": [
        "class PatchEmbed(nn.Module):\n",
        "  def __init__(self, img_size, patch_size, in_chans = 3 , embed_dim = 768):     # in_chans is  no. of input channels (rgb image = 3) # embed_dim (constant) ...int                                                                                                       \n",
        "    super().__init__()\n",
        "    self.img_size = img_size # size of image (square) ....int\n",
        "    self.patch_size = patch_size  # size of patch (square)...int\n",
        "    self.n_patches = (img_size // patch_size) ** 2 # no. of patches inside image\n",
        "    self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size) # convolution layer that does both splitting into pathces and their embedding.\n",
        "\n",
        "  def forward(self, x): # x is a param , input (torch.Tensor), Shape (n_samples, in_chans, img_size, img_size)\n",
        "    x = self.proj(x) # (n_samples, embed_dim, n_patches ** 0.5, n_patches ** 0.5)\n",
        "    x = x.flatten(2) # (n_samples, embed_dim, n_patches) # take last 2 dim and flatten them into single dim\n",
        "    x = x.transpose(1,2) # (n_samples, n_patches, embed_dim) # output\n",
        "    return x"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **2. Attention:**"
      ],
      "metadata": {
        "id": "ygsJiKAIGxCg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Attention(nn.Module):\n",
        "  '''\n",
        "  PARAMETER\n",
        "  ----------------\n",
        "   dim : (int) The input and out dim of per token feature.\n",
        "   n_heads : (int) no. of attention heads.\n",
        "   qkv_bias : (bool) if True, we include bias in linear proj to the query, key, value projections.\n",
        "   attn_p : (float) dropout prob applied to query, key, value tensors.\n",
        "   proj_p : (float) dropout prob applied to output tensors.\n",
        "  \n",
        "  ATTRIBUTES\n",
        "  --------------------\n",
        "  scale : (float) normalizing constant for the dot product.\n",
        "  qkv : (nn.Linear) Linear projection for query, key, value.\n",
        "  proj : (nn.Linear) Linear mapping that takes in the concatenated o/p of all heads and map it into a new space.\n",
        "  attn_drop, proj_drop : (nn.Dropout) Dropout layers.\n",
        "  \n",
        "  '''\n",
        "\n",
        "  def __init__(self, dim, n_heads = 12, qkv_bias = True , attn_p = 0., proj_p = 0.):\n",
        "    super().__init__()\n",
        "    self.n_heads = n_heads\n",
        "    self.dim = dim\n",
        "    self.head_dim = dim // n_heads\n",
        "    self.scale = self.head_dim ** -0.5   # from \"Attention is All you Need\" paper # not to feed extremely large values to softmax , that could lead small gradients.\n",
        "    self.qkv = nn.Linear(dim, dim * 3, bias = qkv_bias) # take token embedding, lineary project them and generate query, key, value.\n",
        "    self.attn_drop = nn.Dropout(attn_p) \n",
        "    self.proj = nn.Linear(dim,dim)\n",
        "    self.proj_drop = nn.Dropout(proj_p)\n",
        "\n",
        "  def forward(self,x):\n",
        "    '''\n",
        "    PARAMETERS:\n",
        "    ------------\n",
        "    x : input, (torch.Tensor), Shape (n_samples, n_pathces + 1, dim)   \n",
        "\n",
        "    RETURNS: (torch.Tensor), Shape (n_samples, n_pathces + 1, dim)\n",
        "    --------\n",
        "    **** input and output tensors have same shape . ***\n",
        "    **** we take 2nd dim as n_patches +1 ; +1 because classtoken is taken as first token in input sequence. . ***\n",
        "    '''\n",
        "\n",
        "    n_samples, n_tokens, dim = x.shape\n",
        "    if dim != self.dim:\n",
        "      raise ValueError\n",
        "\n",
        "    qkv = self.qkv(x) # (n_samples, n_pathces + 1, 3 * dim)   \n",
        "    qkv = qkv.reshape(n_samples, n_tokens, 3, self.n_heads, self.head_dim)  # (n_samples, n_pathces + 1, 3, n_heads, head_dim) # apply LL to 3-d tensor\n",
        "    qkv = qkv.permute(2,0,3,1,4)  # (3, n_samples, n_heads,n_pathces + 1,head_dim) # apply LL to 3-d tensor\n",
        "    q, k, v = qkv[0], qkv[1], qkv[2]  # extract key, query and value\n",
        "    k_t = k.transpose(-2,-1) # (n_samples, n_heads, head_dim, n_patches + 1)\n",
        "    dp = (q @ k_t) * self.scale # (n_samples, n_heads, n_patches + 1, n_patches + 1)\n",
        "    attn = dp.softmax(dim=-1) # (n_samples, n_heads, n_patches + 1, n_patches + 1) \n",
        "    attn = self.attn_drop(attn) \n",
        "    weighted_avg = attn @ v # (n_samples, n_heads,n_pathces + 1,head_dim) \n",
        "    weighted_avg = weighted_avg.transpose(1,2) # (n_samples,n_pathces + 1,n_heads,head_dim) \n",
        "    weighted_avg = weighted_avg.flatten(2) # (n_samples, n_patches + 1, dim)\n",
        "    x = self.proj(weighted_avg) # (n_samples, n_patches + 1, dim)\n",
        "    x = self.proj_drop(x) # (n_samples, n_patches + 1, dim)\n",
        "\n",
        "    return x\n"
      ],
      "metadata": {
        "id": "VKUgqJsaGpzt"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **3. Multi Layer Perceptron:**"
      ],
      "metadata": {
        "id": "WZyqwBA0DLH8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MLP(nn.Module):\n",
        "  ''' Multilayer perceptron.\n",
        "\n",
        "  PARAMETERS :\n",
        "  ------------\n",
        "  in_features : (int) no. of input features.\n",
        "  hidden_features : (int) no. of hidden features.\n",
        "  out_features : (int) no. of output features.\n",
        "  p : (float) Dropout probability.\n",
        "\n",
        "  ATTRIBUTES:\n",
        "  -----------\n",
        "  fc1 : (nn.Linear) first linear layer.\n",
        "  act : (nn.GELU) GELU activation function. # Gaussian Error Linear Unit \n",
        "  fc2 : (nn.Linear) second linear layer.\n",
        "  drop : (nn.Dropout) dropout layer.\n",
        "\n",
        "  '''\n",
        "\n",
        "  def __init__(self, in_features, hidden_features, out_features, p = 0.):\n",
        "    super().__init__()\n",
        "    self.fc1 = nn.Linear(in_features, hidden_features)\n",
        "    self.act = nn.GELU()\n",
        "    self.fc2 = nn.Linear(hidden_features,out_features)\n",
        "    self.drop = nn.Dropout(p)\n",
        "\n",
        "\n",
        "  def forward(self, x):\n",
        "    '''\n",
        "    PARAMETERS:\n",
        "    ------------\n",
        "    x : input, (torch.Tensor), Shape (n_samples, n_pathces + 1, in_features)   \n",
        "\n",
        "    RETURNS: (torch.Tensor), Shape (n_samples, n_pathces + 1, out_features)\n",
        "    --------'''\n",
        "\n",
        "    x = self.fc1(x) # (n_samples, n_pathces + 1, hidden_features)\n",
        "    x = self.act(x) # (n_samples, n_pathces + 1, hidden_features)\n",
        "    x = self.drop(x) # (n_samples, n_pathces + 1, hidden_features)\n",
        "    x = self.fc2(x) # (n_samples, n_pathces + 1, hidden_features)\n",
        "    x = self.drop(x) # (n_samples, n_pathces + 1, hidden_features)\n",
        "\n",
        "    return x"
      ],
      "metadata": {
        "id": "TEQGttnLnVYu"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **4. Transformer Block:**"
      ],
      "metadata": {
        "id": "8TPSe4BtDLTj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Block(nn.Module):\n",
        "  \"\"\" Transformer block.\n",
        "  \n",
        "  PARAMETERS:\n",
        "  -----------\n",
        "  dim : (int) Embedding Dimension.\n",
        "  n_heads : (int) No. of attention heads.\n",
        "  mlp_ratio : (float) determines the hidden dm size of 'MLP' module wrt 'dim'.\n",
        "  qkv_bias : (bool) if True, we include bias in linear proj to the query, key, value projections.\n",
        "  attn_p : (float) dropout prob applied to query, key, value tensors.\n",
        "   p : (float) Dropout probability.\n",
        "\n",
        "  ATTRIBUTES:\n",
        "  -----------\n",
        "  norm1, norm2 : (LayerNorm) Layer Normalization.\n",
        "  attn : (Attention)  Attention module.\n",
        "  mlp : (MLP) MLP module.\n",
        "  \"\"\" \n",
        "  def __init__(self, dim, n_heads, mlp_ratio = 4.0, qkv_bias = True, p = 0., attn_p = 0.):\n",
        "    super().__init__()\n",
        "    self.norm1 = nn.LayerNorm(dim, eps=1e-6)\n",
        "    self.attn = Attention( dim, n_heads = n_heads, qkv_bias = qkv_bias , attn_p = attn_p, proj_p = p)\n",
        "    self.norm2 = nn.LayerNorm(dim, eps=1e-6)\n",
        "    hidden_features = int(dim * mlp_ratio)\n",
        "    self.mlp = MLP(in_features=dim,hidden_features=hidden_features, out_features=dim )\n",
        "\n",
        "  def forward(self, x):\n",
        "    '''PARAMETERS:\n",
        "    ------------\n",
        "    x : input, (torch.Tensor), Shape (n_samples, n_pathces + 1, dim)   \n",
        "\n",
        "    RETURNS: (torch.Tensor), Shape (n_samples, n_pathces + 1, dim)\n",
        "    --------'''\n",
        "    # both norm is having saperate parameters.\n",
        "    x = x + self.attn(self.norm1(x))\n",
        "    x = x + self.mlp(self.norm2(x)) \n",
        "\n",
        "    return x"
      ],
      "metadata": {
        "id": "w80EL6jFq307"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **5. Vision Transformer:**"
      ],
      "metadata": {
        "id": "8Cq28YSADLgz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class VisionTransformer(nn.Module):\n",
        "  '''\n",
        "  PARAMETERS:\n",
        "  ------------\n",
        "  img_size : (int) height and width of image (square).\n",
        "  patch_size : (int) height and width of patch (square).\n",
        "  in_chans : (int) no. of input channels.\n",
        "  n_classes: (int) no. of classes.\n",
        "  embed_dim : (int) Dimensionality of the token/patch embeddings.\n",
        "  depth : (int) no. of blocks.\n",
        "  n_heads : (int) no. of attention heads.\n",
        "  mlp_ratio : (float) determines the hidden dimention of mlp module.\n",
        "  qkv_bias : (bool) if True, we include bias in linear proj to the query, key, value projections.\n",
        "  attn_p : (float) dropout prob applied to query, key, value tensors.\n",
        "  p : (float) Dropout probability.\n",
        "\n",
        "  ATTRIBUTES:\n",
        "  -----------\n",
        "  patch_embed : (PatchEmbed) Instance of 'PatchEmbed' layer.\n",
        "\n",
        "  cls_token : (nn.Parameter) Learnable parameter that will represnt the first token in sequence.\n",
        "  It has 'embed_dim' elements.\n",
        "\n",
        "  pos_emb : (nn.Parameter) Positional embedding of the cls token + all the patches.\n",
        "  It has '(n_patches + 1) * embed_dim' elements.\n",
        "\n",
        "  pos_drop : (nn.Dropout) Dropout layer.\n",
        "  blocks : (nn.ModuleList) List of 'Block' modules.\n",
        "  norm : (nn.LayerNorm) Layer normalization.\n",
        "  '''\n",
        "  def __init__(self, \n",
        "               img_size=384,\n",
        "               patch_size=16,\n",
        "               in_chans=3,\n",
        "               n_classes=1000,\n",
        "               embed_dim=768,\n",
        "               depth=12,\n",
        "               n_heads=12,\n",
        "               mlp_ratio=4,\n",
        "               qkv_bias=True,\n",
        "               p=0.,\n",
        "               attn_p=0.):\n",
        "      super().__init__()\n",
        "      self.patch_embed = PatchEmbed(img_size=img_size, patch_size=patch_size, in_chans=in_chans, embed_dim=embed_dim)\n",
        "      self.cls_token = nn.Parameter(torch.zeros(1,1,embed_dim)) # initialize with zeroes\n",
        "      self.pos_embed = nn.Parameter(torch.zeros(1,1 + self.patch_embed.n_patches,embed_dim))\n",
        "      self.pos_drop = nn.Dropout(p=p)\n",
        "      self.blocks = nn.ModuleList(\n",
        "          [\n",
        "          Block(\n",
        "              dim=embed_dim,\n",
        "              n_heads=n_heads,\n",
        "              mlp_ratio=mlp_ratio,\n",
        "              qkv_bias=qkv_bias,\n",
        "              p=p,\n",
        "              attn_p=attn_p,\n",
        "          )\n",
        "          for _ in range(depth)\n",
        "          ]\n",
        "      )  \n",
        "\n",
        "      self.norm = nn.LayerNorm(embed_dim, eps=1e-6)\n",
        "      self.head = nn.Linear(embed_dim, n_classes)\n",
        "\n",
        "  def forward(self, x):\n",
        "    '''PARAMETERS:\n",
        "    ------------\n",
        "    x : input, (torch.Tensor), Shape (n_samples, in_chans, img_size, img_size)   \n",
        "    RETURNS: logits: (torch.Tensor), logits over all the classes, Shape (n_samples, n_classes)\n",
        "    --------\n",
        "    '''\n",
        "    n_samples = x.shape[0]\n",
        "    x = self.patch_embed(x)\n",
        "\n",
        "    cls_token = self.cls_token.expand(n_samples, -1, -1)  # (n_sample, 1, embed_dim)\n",
        "    x = torch.cat((cls_token, x), dim = 1) # (n_samples, 1 + n_patches, embed_dim) # append cls_token at front in x.\n",
        "    x = x + self.pos_embed # (n_samples, 1 + n_patches, embed_dim)\n",
        "    x = self.pos_drop(x)\n",
        "\n",
        "    for block in self.blocks: # apply all the blocks of encoder on x\n",
        "      x = block(x) \n",
        "\n",
        "    x = self.norm(x) # apply layer normalization\n",
        "\n",
        "    cls_token_final = x[:,0] # out of all embedding we only select class embedding (CLS token)\n",
        "    x = self.head(cls_token_final) # classifier # This embedding encode meaning of entire image\n",
        "\n",
        "    return x\n"
      ],
      "metadata": {
        "id": "FWh0aqbLDIuu"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Testing part:**"
      ],
      "metadata": {
        "id": "jEJCX02HNcoh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install timm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BFhvhbNRN8W9",
        "outputId": "26e04846-2191-4387-fb0e-18e25bc46ac2"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: timm in /usr/local/lib/python3.7/dist-packages (0.5.4)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (from timm) (0.11.1+cu111)\n",
            "Requirement already satisfied: torch>=1.4 in /usr/local/lib/python3.7/dist-packages (from timm) (1.10.0+cu111)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.4->timm) (3.10.0.2)\n",
            "Requirement already satisfied: pillow!=8.3.0,>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision->timm) (7.1.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchvision->timm) (1.21.5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import timm\n",
        "import torch\n",
        "#from custom import VisionTransformer"
      ],
      "metadata": {
        "id": "dUBXft2ZNb7A"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Helper function \n",
        "def get_n_param(module):\n",
        "  return sum(p.numel() for p in module.parameters() if p.requires_grad) # to count no. of elements\n",
        "\n",
        "def assert_tensors_equal(t1,t2): # check the 2 tensors are equal or not.\n",
        "  a1,a2 = t1.detach().numpy(), t2.detach().numpy()\n",
        "  np.testing.assert_allclose(a1,a2)\n",
        "\n",
        "# load pretrained vision transformer models from timm\n",
        "model_name = \"vit_base_patch16_384\" \n",
        "model_official = timm.create_model(model_name, pretrained = True)\n",
        "model_official.eval()\n",
        "print(type(model_official))\n",
        "\n",
        "# declare hyperparameters corruspoding to pretrained model\n",
        "custom_config = {\n",
        "    \"img_size\" : 384,\n",
        "    \"patch_size\": 16,\n",
        "    \"in_chans\": 3,\n",
        "    # n_classes=1000,\n",
        "    \"embed_dim\": 768,\n",
        "    \"depth\":12,\n",
        "    \"n_heads\": 12,\n",
        "    \"mlp_ratio\": 4,\n",
        "    \"qkv_bias\": True\n",
        "    }\n",
        "\n",
        "# instantiate the implemented custom model and set it to evaluation mode\n",
        "model_custom = VisionTransformer(**custom_config)\n",
        "model_custom.eval() \n",
        "\n",
        "# iterate through all the parameters of official and custom network\n",
        "for (n_o,p_o), (n_c,p_c) in zip(model_official.named_parameters(),model_custom.named_parameters()):\n",
        "  assert p_o.numel() == p_c.numel() # first we check for each parameters no. of elements are equal.\n",
        "  #print(\"{n_o} | {n_c}\")\n",
        "\n",
        "  p_c.data[:] = p_o.data\n",
        "\n",
        "  assert_tensors_equal(p_c.data, p_o.data)\n",
        "\n",
        "inp = torch.rand(1,3,384,384) # create a random tensor with exact size\n",
        "# run forward pass\n",
        "res_c = model_custom(inp)\n",
        "res_o = model_official(inp)\n",
        "\n",
        "# Asserts\n",
        "assert get_n_param(model_custom) == get_n_param(model_official) # check no. of parameter for both models\n",
        "assert_tensors_equal(res_c, res_o) # take 2 tensors, make sure they are identical\n",
        "\n",
        "# save custom model\n",
        "torch.save(model_custom,\"TransformerModel.pth\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Xc_VdECOaea",
        "outputId": "ed232a04-a01c-4e6b-a08a-c787a2f29a44"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'timm.models.vision_transformer.VisionTransformer'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!git clone https://github.com/lucidrains/vit-pytorch"
      ],
      "metadata": {
        "id": "BtTf9akQjOXa"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **get prediction:**"
      ],
      "metadata": {
        "id": "u8DJaiTAAY3J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from PIL import Image\n",
        "import torch\n",
        "import cv2 \n",
        "k = 10\n",
        "\n",
        "#imagenet_labels = dict(enumerate(open(\"classes.txt\")))\n",
        "\n",
        "model = torch.load(\"TransformerModel.pth\")\n",
        "model.eval()\n",
        "\n",
        "def prediction(image):\n",
        "  # new image loading\n",
        "  img = cv2.resize(cv2.imread(image), (384, 384)) \n",
        "  img = (np.array(img) / 128) - 1 # in the range -1, 1\n",
        "  inp = torch.from_numpy(img).permute(2,0,1).unsqueeze(0).to(torch.float32)\n",
        "  logits = model(inp)\n",
        "  probs = torch.nn.functional.softmax(logits, dim=1)\n",
        "  #print(probs.shape)\n",
        "  op = (torch.argmax(probs)).item()\n",
        "  print(\"the predicted class index is:\", op)\n",
        "  t = []\n",
        "  with open('classes.txt', encoding='utf8') as f:\n",
        "      for line in f:\n",
        "          t.append(line.strip())\n",
        "  print(\"the predicted class is:\", t[op])"
      ],
      "metadata": {
        "id": "PM3ewe60laEg"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prediction(\"cat.jpg\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lc44xLl8AVFS",
        "outputId": "7d1f880e-b498-4d9a-9bb2-0f09baebac16"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "the predicted class index is: 281\n",
            "the predicted class is: 281: 'tabby, tabby cat',\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prediction(\"dog.png\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T-dtRw4mA5hK",
        "outputId": "c1d3c724-1947-4951-979c-674341b93cc7"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "the predicted class index is: 151\n",
            "the predicted class is: 151: 'Chihuahua',\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prediction(\"dog2.png\") # adv dog"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZeJS4_DlAp_T",
        "outputId": "7eecfa06-3068-48d0-b8c4-b8c752f84200"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "the predicted class index is: 850\n",
            "the predicted class is: 850: 'teddy, teddy bear',\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **How linear layer behave, when we have 3-dim or more dim tensors!!**"
      ],
      "metadata": {
        "id": "SWuWV5k6tPdf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "module = torch.nn.Linear(10,20)\n",
        "module"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F3aM1NArsygr",
        "outputId": "22f8db92-fc26-415e-9d50-07d07146f540"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Linear(in_features=10, out_features=20, bias=True)"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "n_samples = 40 \n",
        "in_2d = torch.rand(n_samples,10) # last dim must be same as declared in module's linear layer 'in_feature'\n",
        "module(in_2d).shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "awGVLjwGs9h1",
        "outputId": "f84ce8b9-5043-4f43-d72d-388cdee9e667"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([40, 20])"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "in_3d = torch.rand(n_samples,33,10) # last dim must be same as declared in module's linear layer 'in_feature'\n",
        "module(in_3d).shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "maCKkj_UtIAN",
        "outputId": "82027694-a497-43b5-9ada-ea3f2eb9d2fc"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([40, 33, 20])"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "in_5d = torch.rand(n_samples,2,3,4,10) # last dim must be same as declared in module's linear layer 'in_feature'\n",
        "module(in_5d).shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M-331iH2uODE",
        "outputId": "d802f22c-d3cf-4075-f7be-efe39ef6c180"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([40, 2, 3, 4, 20])"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "in_7d = torch.rand(n_samples,2,3,4,5,6,10) # last dim must be same as declared in module's linear layer 'in_feature'\n",
        "module(in_7d).shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y53xZEG2ucM7",
        "outputId": "46775211-93e8-4801-b78c-3da154209bc3"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([40, 2, 3, 4, 5, 6, 20])"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Basic Property of Layer Normalization:**\n",
        "Layernorm normalize data for each sample."
      ],
      "metadata": {
        "id": "sv3inr_kPoEk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch"
      ],
      "metadata": {
        "id": "juAsuXgrutM7"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inp = torch.tensor([[0,4.], [-1,7],[3,5]])"
      ],
      "metadata": {
        "id": "dEJEH5D3P3gm"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_samples, n_features = inp.shape"
      ],
      "metadata": {
        "id": "7vzoqykhQCTd"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "module = torch.nn.LayerNorm(n_features,elementwise_affine=False)"
      ],
      "metadata": {
        "id": "OtphWdEpQHGV"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sum(p.numel() for p in module.parameters() if p.requires_grad) # 0 lernable parameter"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TwUzLhOeWAJ5",
        "outputId": "c83a894f-6178-4ece-d4d3-de9e3c5183fc"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "inp.mean(-1),inp.std(-1,unbiased= False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xbWax8xKQo0t",
        "outputId": "3ca2cd72-d8f6-4f93-b8db-48397c46bd5e"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([2., 3., 4.]), tensor([2., 4., 1.]))"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "module(inp).mean(-1),module(inp).std(-1,unbiased= False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4589yYUaQw7U",
        "outputId": "7f0b82d4-cb83-4dd5-eca6-13df1809d293"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([0., 0., 0.]), tensor([1.0000, 1.0000, 1.0000]))"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "module = torch.nn.LayerNorm(n_features,elementwise_affine=True)"
      ],
      "metadata": {
        "id": "lb45vYc3Vvek"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sum(p.numel() for p in module.parameters() if p.requires_grad) # 4 lernable parameter"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_haY4cCaXPuT",
        "outputId": "d8767434-c145-4c4c-f2c4-16cf275d8d55"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "4 learnable parameter , contains bias and weight parameter of module."
      ],
      "metadata": {
        "id": "jmgGySrxYCer"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "module.bias, module.weight"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t8ipYspTXTFq",
        "outputId": "b0149e23-3129-4772-ef61-ae8b460d14c4"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(Parameter containing:\n",
              " tensor([0., 0.], requires_grad=True), Parameter containing:\n",
              " tensor([1., 1.], requires_grad=True))"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "module(inp).mean(-1),module(inp).std(-1,unbiased= False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C9h-qrYRX5v6",
        "outputId": "6d6c1d46-13c7-4013-ac88-0a0a498a380b"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([0., 0., 0.], grad_fn=<MeanBackward1>),\n",
              " tensor([1.0000, 1.0000, 1.0000], grad_fn=<StdBackward0>))"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "module(torch.rand(n_samples,2,3,4,5,6,n_features)).shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1B0nrq_4Ys55",
        "outputId": "21a509bf-d01b-475f-bbaa-a32994455ef9"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([3, 2, 3, 4, 5, 6, 2])"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "module(torch.rand(n_samples,2,3,4,5,6,n_features)).mean(-1) # -1 becoz always last dim being norm."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LMv4o6lrZaIx",
        "outputId": "a85eca33-5659-4f86-cb4c-64337d58c34d"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[[[[ 0.0000e+00,  6.2585e-07,  5.9605e-08,  2.9802e-08,  1.4901e-07,\n",
              "              0.0000e+00],\n",
              "            [ 5.9605e-08, -2.9802e-08, -5.9605e-08, -2.9802e-08,  2.9802e-08,\n",
              "             -5.9605e-08],\n",
              "            [ 1.4901e-07, -2.9802e-08, -2.9802e-08, -1.1921e-07, -2.9802e-08,\n",
              "             -8.9407e-08],\n",
              "            [-1.4901e-07,  8.9407e-08, -2.0862e-07, -1.7881e-07,  5.9605e-08,\n",
              "             -2.9802e-08],\n",
              "            [-1.2517e-06, -2.0862e-07, -5.9605e-08,  1.1921e-07,  2.9802e-07,\n",
              "              2.9802e-08]],\n",
              "\n",
              "           [[-2.9802e-07, -6.0201e-06,  5.9605e-08, -2.3842e-07,  0.0000e+00,\n",
              "             -8.9407e-08],\n",
              "            [-5.9605e-08,  5.9605e-08, -2.3842e-07,  8.9407e-08, -1.1921e-07,\n",
              "             -2.9802e-08],\n",
              "            [ 2.9802e-07,  1.1921e-07, -2.9802e-07,  0.0000e+00, -1.1921e-07,\n",
              "              1.1921e-07],\n",
              "            [-2.9802e-08,  2.3842e-07, -2.9802e-08,  0.0000e+00,  4.4703e-07,\n",
              "              2.9802e-08],\n",
              "            [-2.0862e-07, -1.7881e-07,  5.9605e-08,  2.9802e-08, -1.7881e-07,\n",
              "              5.9605e-08]],\n",
              "\n",
              "           [[-2.9802e-08, -7.7486e-07,  0.0000e+00, -1.7881e-07,  8.9407e-08,\n",
              "              1.1921e-07],\n",
              "            [-2.9802e-08,  0.0000e+00,  2.9802e-08,  0.0000e+00, -8.9407e-08,\n",
              "             -4.1723e-07],\n",
              "            [ 0.0000e+00, -2.6822e-07,  2.3842e-07,  1.4901e-07, -5.9605e-08,\n",
              "              8.9407e-08],\n",
              "            [-1.1921e-07,  0.0000e+00, -2.9802e-08,  8.9407e-08, -2.9802e-07,\n",
              "              8.9407e-08],\n",
              "            [ 0.0000e+00,  0.0000e+00,  8.9407e-08,  5.9605e-08, -1.4901e-07,\n",
              "              8.9407e-08]],\n",
              "\n",
              "           [[ 5.3644e-07,  0.0000e+00,  0.0000e+00,  5.9605e-08,  0.0000e+00,\n",
              "             -3.5763e-07],\n",
              "            [ 5.9605e-08,  5.3644e-07, -2.6822e-07, -2.0862e-07,  0.0000e+00,\n",
              "             -1.1921e-07],\n",
              "            [-2.9802e-08, -5.9605e-08,  5.9605e-08,  5.9605e-08,  5.9605e-08,\n",
              "             -5.9605e-08],\n",
              "            [ 2.9802e-08,  8.9407e-08, -3.2783e-07, -1.1921e-07, -2.4736e-06,\n",
              "              2.3842e-07],\n",
              "            [ 1.4901e-07,  0.0000e+00, -2.0862e-07, -2.9802e-08,  1.7881e-07,\n",
              "              8.9407e-08]]],\n",
              "\n",
              "\n",
              "          [[[-1.1921e-07,  8.9407e-08,  6.5565e-07, -2.9802e-08,  2.0862e-07,\n",
              "              2.9802e-08],\n",
              "            [ 2.9802e-08,  8.9407e-07,  0.0000e+00, -2.9802e-08, -8.9407e-08,\n",
              "              5.9605e-08],\n",
              "            [ 1.4901e-08, -5.9605e-08,  2.3842e-07, -2.9802e-08,  1.7881e-07,\n",
              "             -2.9802e-08],\n",
              "            [ 1.1921e-07,  7.1526e-07, -8.9407e-08, -5.9605e-08,  1.7881e-07,\n",
              "              1.3113e-06],\n",
              "            [ 8.9407e-08,  2.9802e-08,  6.5565e-07,  8.3447e-07,  0.0000e+00,\n",
              "             -2.9802e-08]],\n",
              "\n",
              "           [[-2.0862e-07, -2.3842e-07,  7.1526e-07, -5.9605e-08,  1.1921e-07,\n",
              "              0.0000e+00],\n",
              "            [ 0.0000e+00,  6.8545e-07,  0.0000e+00,  5.9605e-08, -2.9802e-08,\n",
              "             -5.9605e-08],\n",
              "            [-3.2783e-07,  5.9605e-08,  2.9802e-08, -2.9802e-08, -5.9605e-08,\n",
              "              5.0664e-07],\n",
              "            [ 5.9605e-08,  2.9802e-08, -2.9802e-08, -2.9802e-08,  8.9407e-08,\n",
              "             -2.3842e-07],\n",
              "            [-1.1921e-07,  0.0000e+00,  1.4901e-07, -1.7881e-06, -5.9605e-08,\n",
              "             -5.9605e-08]],\n",
              "\n",
              "           [[ 5.9605e-08, -2.0862e-07,  0.0000e+00,  8.9407e-08,  2.9802e-08,\n",
              "              1.6391e-06],\n",
              "            [-5.9605e-08,  2.4140e-06, -5.9605e-08, -1.1921e-06,  1.7881e-07,\n",
              "              0.0000e+00],\n",
              "            [-2.9802e-08, -1.3411e-06,  2.9802e-08, -2.9802e-08, -9.8348e-07,\n",
              "             -8.9407e-08],\n",
              "            [ 1.1921e-07,  2.9802e-08,  1.7881e-07, -8.9407e-08, -2.3842e-07,\n",
              "             -1.4901e-07],\n",
              "            [ 0.0000e+00, -3.5763e-07, -5.9605e-08,  3.6955e-06, -8.9407e-08,\n",
              "              4.4703e-07]],\n",
              "\n",
              "           [[ 8.9407e-08, -7.1526e-07,  5.9605e-08,  5.9605e-08,  5.9605e-08,\n",
              "             -8.0466e-07],\n",
              "            [ 1.4901e-07,  0.0000e+00,  4.1723e-07, -5.9605e-08,  0.0000e+00,\n",
              "              1.4901e-07],\n",
              "            [ 2.0862e-07,  0.0000e+00,  2.9802e-08,  0.0000e+00,  2.3842e-07,\n",
              "              4.4703e-07],\n",
              "            [ 0.0000e+00,  8.9407e-08, -2.9802e-08,  4.1723e-07, -5.9605e-08,\n",
              "              3.5763e-07],\n",
              "            [ 1.5497e-06,  0.0000e+00,  5.9605e-08, -1.4901e-07,  8.9407e-08,\n",
              "             -1.6391e-06]]],\n",
              "\n",
              "\n",
              "          [[[ 5.9605e-08,  2.9802e-08,  5.9605e-08,  5.9605e-08,  0.0000e+00,\n",
              "              2.9802e-08],\n",
              "            [ 2.3842e-07,  2.0862e-07, -1.4901e-07,  6.8545e-07, -2.9802e-08,\n",
              "             -1.1921e-07],\n",
              "            [ 5.9605e-08, -2.9802e-08,  2.9802e-08, -5.9605e-08, -2.0862e-07,\n",
              "             -1.1921e-07],\n",
              "            [-8.9407e-08, -5.9605e-08,  0.0000e+00,  2.0862e-07, -5.9605e-08,\n",
              "             -5.9605e-08],\n",
              "            [-2.9802e-08,  2.0862e-07, -5.9605e-08, -2.9802e-08, -2.9802e-08,\n",
              "              2.9802e-08]],\n",
              "\n",
              "           [[-2.9802e-08,  2.9802e-08,  8.9407e-08,  2.3842e-07,  1.7881e-07,\n",
              "              3.1590e-06],\n",
              "            [ 5.9605e-08, -3.5763e-07,  1.4901e-07,  2.0862e-07,  0.0000e+00,\n",
              "             -8.0466e-07],\n",
              "            [-1.1623e-06, -2.9802e-08,  2.9802e-08,  2.9802e-08,  2.9802e-08,\n",
              "              1.5795e-06],\n",
              "            [-8.9407e-08, -3.5763e-07,  0.0000e+00, -1.4901e-07, -8.9407e-08,\n",
              "              0.0000e+00],\n",
              "            [ 2.9802e-08,  0.0000e+00, -5.9605e-08, -2.0862e-07, -8.9407e-08,\n",
              "             -5.6624e-07]],\n",
              "\n",
              "           [[ 1.7881e-07, -2.9802e-08, -2.9802e-08, -1.7881e-07,  1.4901e-07,\n",
              "             -5.9605e-08],\n",
              "            [-2.0862e-07,  1.7881e-07,  4.1723e-07, -1.1921e-07, -4.4703e-07,\n",
              "              0.0000e+00],\n",
              "            [ 8.9407e-08,  2.9802e-08, -4.1723e-07, -1.1921e-07, -2.9802e-08,\n",
              "             -1.4901e-07],\n",
              "            [ 0.0000e+00,  2.9802e-08,  2.9802e-08, -4.1723e-07,  0.0000e+00,\n",
              "              5.9605e-07],\n",
              "            [-5.9605e-08,  7.7486e-07,  2.9802e-08,  5.9605e-08,  4.4703e-07,\n",
              "              5.9605e-08]],\n",
              "\n",
              "           [[ 0.0000e+00, -8.9407e-08, -2.9802e-08,  2.9802e-08,  0.0000e+00,\n",
              "             -1.4901e-07],\n",
              "            [ 2.9802e-08,  0.0000e+00, -5.9605e-08, -2.9802e-08, -5.9605e-08,\n",
              "              2.9802e-08],\n",
              "            [ 0.0000e+00, -3.8743e-07,  2.0862e-07, -2.9802e-08, -2.9802e-08,\n",
              "             -2.8908e-06],\n",
              "            [ 8.6427e-07, -8.9407e-08, -2.0862e-07, -5.9605e-08, -8.6427e-07,\n",
              "             -1.7881e-07],\n",
              "            [ 3.5763e-07,  4.4703e-07, -2.9802e-08, -2.0862e-07,  5.9605e-08,\n",
              "              4.4703e-07]]]],\n",
              "\n",
              "\n",
              "\n",
              "         [[[[ 1.1921e-07, -8.9407e-08,  1.7881e-07,  2.9802e-08,  0.0000e+00,\n",
              "              1.7881e-07],\n",
              "            [ 2.9802e-08,  5.9605e-08,  8.9407e-08,  2.3842e-07,  0.0000e+00,\n",
              "              1.7881e-07],\n",
              "            [ 8.9407e-08,  8.9407e-08,  1.1921e-07, -9.5367e-07,  0.0000e+00,\n",
              "              0.0000e+00],\n",
              "            [ 0.0000e+00,  2.9802e-08, -3.2783e-07,  1.1921e-07, -2.6822e-07,\n",
              "             -8.7321e-06],\n",
              "            [ 1.1921e-07,  1.7881e-07, -2.3842e-07, -2.9802e-08,  0.0000e+00,\n",
              "              2.0862e-07]],\n",
              "\n",
              "           [[-2.9802e-08,  5.9605e-08,  8.9407e-08, -5.9605e-08,  3.5763e-07,\n",
              "             -2.6822e-07],\n",
              "            [-1.1921e-07,  2.3842e-07,  2.9802e-08, -2.9802e-08, -2.9802e-08,\n",
              "             -1.1921e-07],\n",
              "            [ 1.1921e-07, -1.1921e-07,  0.0000e+00, -1.4901e-07,  2.3842e-07,\n",
              "             -2.9802e-08],\n",
              "            [ 2.9802e-08,  0.0000e+00, -1.7881e-07,  3.5763e-07,  5.9605e-08,\n",
              "             -2.3842e-07],\n",
              "            [-8.9407e-08, -5.9605e-08, -8.9407e-08,  0.0000e+00, -4.4703e-07,\n",
              "              0.0000e+00]],\n",
              "\n",
              "           [[ 0.0000e+00,  0.0000e+00,  5.9605e-08,  2.9802e-08, -8.9407e-08,\n",
              "             -2.9802e-08],\n",
              "            [ 1.3113e-06, -2.0862e-07, -5.9605e-08,  0.0000e+00, -5.9605e-08,\n",
              "              1.4901e-07],\n",
              "            [-8.9407e-08, -2.9802e-08,  2.9802e-08,  2.0862e-07, -1.1921e-07,\n",
              "             -2.9802e-07],\n",
              "            [-2.9802e-08,  5.9605e-08, -2.9802e-08, -5.9605e-07,  1.1921e-07,\n",
              "              2.9802e-08],\n",
              "            [ 0.0000e+00, -2.9802e-08,  2.9802e-08,  3.8743e-07, -2.9802e-08,\n",
              "              1.7881e-07]],\n",
              "\n",
              "           [[-9.5367e-07,  5.9605e-08,  0.0000e+00,  1.4007e-06,  5.9605e-08,\n",
              "             -4.7684e-07],\n",
              "            [ 2.9802e-08,  0.0000e+00, -1.1921e-07, -2.9802e-08, -1.1921e-07,\n",
              "             -8.9407e-08],\n",
              "            [ 2.9802e-08,  5.9605e-08,  5.9605e-08,  0.0000e+00,  0.0000e+00,\n",
              "              2.0862e-07],\n",
              "            [ 5.9605e-08,  1.1921e-07, -2.9802e-08, -2.9802e-07, -8.9407e-08,\n",
              "              3.8743e-07],\n",
              "            [-2.9802e-08,  5.9605e-08, -5.9605e-08, -2.9802e-08, -2.9802e-08,\n",
              "              1.4901e-07]]],\n",
              "\n",
              "\n",
              "          [[[-3.8743e-07, -2.9802e-08,  0.0000e+00, -5.9605e-08, -1.1921e-07,\n",
              "              2.6822e-07],\n",
              "            [ 2.9802e-08, -1.1921e-06, -5.9605e-08,  5.9605e-08,  0.0000e+00,\n",
              "              6.8545e-07],\n",
              "            [ 1.9968e-06,  5.9605e-08,  0.0000e+00,  2.9802e-08,  5.9605e-08,\n",
              "              1.4901e-07],\n",
              "            [ 5.9605e-08, -5.9605e-08, -1.4901e-07,  1.4901e-07,  0.0000e+00,\n",
              "              1.5795e-06],\n",
              "            [ 5.9605e-08, -2.9802e-08,  0.0000e+00, -5.9605e-08,  0.0000e+00,\n",
              "             -8.9407e-08]],\n",
              "\n",
              "           [[-5.3942e-06,  0.0000e+00, -5.9605e-08, -1.7881e-07, -8.9407e-08,\n",
              "              5.9605e-08],\n",
              "            [ 2.9802e-08, -5.9605e-08, -5.9605e-08,  0.0000e+00,  2.9802e-08,\n",
              "              0.0000e+00],\n",
              "            [-8.9407e-08,  0.0000e+00, -2.0862e-07, -5.9605e-08,  3.5763e-07,\n",
              "             -5.9605e-08],\n",
              "            [-2.9802e-08, -2.9802e-08, -5.9605e-08, -2.9802e-08, -2.9802e-08,\n",
              "              0.0000e+00],\n",
              "            [-3.5763e-07,  0.0000e+00, -1.7881e-07,  9.2387e-07, -2.9802e-08,\n",
              "             -8.9407e-08]],\n",
              "\n",
              "           [[ 2.3842e-07, -5.9605e-08, -8.9407e-08, -2.9802e-08,  5.9605e-08,\n",
              "             -2.3842e-07],\n",
              "            [ 0.0000e+00,  2.9802e-08, -2.9802e-08, -5.9605e-08,  4.3511e-06,\n",
              "              0.0000e+00],\n",
              "            [ 0.0000e+00, -5.9605e-08,  3.7998e-06,  2.3842e-07,  0.0000e+00,\n",
              "              8.9407e-08],\n",
              "            [ 2.9802e-08, -5.9605e-08,  1.1921e-07,  3.8743e-07,  0.0000e+00,\n",
              "              0.0000e+00],\n",
              "            [ 2.9802e-08, -8.9407e-08,  6.5565e-07, -5.3644e-07,  0.0000e+00,\n",
              "              0.0000e+00]],\n",
              "\n",
              "           [[ 2.3842e-07,  1.1921e-07,  4.4703e-07,  1.1921e-07,  2.0862e-07,\n",
              "             -2.9802e-08],\n",
              "            [ 1.4901e-07,  7.1824e-06, -4.7684e-07,  2.9802e-08,  8.9407e-07,\n",
              "             -5.9605e-08],\n",
              "            [-5.9605e-08,  0.0000e+00, -2.9802e-08, -8.9407e-08, -1.4901e-07,\n",
              "             -1.1921e-07],\n",
              "            [ 2.9802e-08, -2.9802e-08,  2.9802e-08,  0.0000e+00,  5.9605e-08,\n",
              "              0.0000e+00],\n",
              "            [-5.9605e-08, -1.1921e-07,  5.9605e-08,  1.6391e-06, -2.9802e-08,\n",
              "              1.4901e-07]]],\n",
              "\n",
              "\n",
              "          [[[-2.0862e-07,  8.9407e-08,  2.9802e-08,  0.0000e+00, -2.9802e-08,\n",
              "              5.9605e-08],\n",
              "            [ 2.9802e-08,  1.1921e-07,  2.9802e-08,  2.6822e-07,  2.3842e-07,\n",
              "              2.9802e-08],\n",
              "            [ 1.7881e-07,  0.0000e+00,  1.1921e-07, -2.0862e-07, -2.9802e-08,\n",
              "              2.3842e-07],\n",
              "            [ 2.9802e-08,  0.0000e+00,  0.0000e+00,  8.9407e-08,  8.0466e-07,\n",
              "              2.3842e-07],\n",
              "            [ 2.0862e-07,  2.9802e-08, -5.9605e-08,  2.0862e-07,  5.9605e-08,\n",
              "             -5.9605e-08]],\n",
              "\n",
              "           [[ 0.0000e+00, -9.2387e-07, -5.9605e-08, -5.6624e-07, -2.9802e-08,\n",
              "             -2.9802e-08],\n",
              "            [ 8.9407e-08, -1.1921e-07,  3.1292e-06, -8.9407e-08, -5.9605e-08,\n",
              "              3.5763e-07],\n",
              "            [-1.7881e-07,  2.9802e-08, -2.9802e-08, -1.4901e-07,  2.9802e-08,\n",
              "              2.9802e-08],\n",
              "            [ 5.6624e-07, -8.6427e-07,  5.9605e-08,  5.9605e-08,  2.3842e-07,\n",
              "              2.9802e-08],\n",
              "            [ 2.9802e-08, -3.5763e-07,  0.0000e+00, -5.9605e-08,  0.0000e+00,\n",
              "              1.3322e-05]],\n",
              "\n",
              "           [[-1.4901e-06, -2.9802e-08, -1.0580e-06,  2.9802e-08,  5.9605e-08,\n",
              "             -2.4289e-06],\n",
              "            [ 2.9802e-08,  0.0000e+00,  2.3842e-07,  1.1921e-07,  1.7881e-07,\n",
              "             -2.0862e-07],\n",
              "            [ 1.4901e-07,  2.9802e-08,  2.9802e-08, -8.9407e-08, -1.4901e-07,\n",
              "             -1.1921e-07],\n",
              "            [ 0.0000e+00,  1.6093e-06,  0.0000e+00, -1.1921e-07, -2.9802e-08,\n",
              "             -5.9605e-08],\n",
              "            [-1.4901e-07,  1.1921e-07,  5.9605e-08, -1.1921e-07, -2.0862e-07,\n",
              "              2.9802e-08]],\n",
              "\n",
              "           [[ 5.9605e-08,  0.0000e+00,  1.1921e-07, -2.9802e-08,  0.0000e+00,\n",
              "              8.9407e-08],\n",
              "            [ 2.9802e-08, -4.4703e-07, -2.9802e-08, -1.7881e-07, -1.4901e-07,\n",
              "              5.9605e-08],\n",
              "            [ 2.0862e-07, -2.9802e-08,  5.9605e-08,  8.9407e-08, -1.2219e-06,\n",
              "             -5.9605e-08],\n",
              "            [ 0.0000e+00, -1.4901e-07,  1.1921e-07,  0.0000e+00,  2.9802e-08,\n",
              "              2.0266e-06],\n",
              "            [ 5.9605e-08, -4.1723e-07,  8.1956e-06,  5.9605e-08,  5.9605e-08,\n",
              "             -2.9802e-08]]]]],\n",
              "\n",
              "\n",
              "\n",
              "\n",
              "        [[[[[-2.9802e-08, -4.1723e-07,  2.6822e-07,  0.0000e+00,  5.9605e-08,\n",
              "             -1.1921e-07],\n",
              "            [ 1.4901e-07,  2.6822e-07,  2.3842e-07,  0.0000e+00, -5.9605e-08,\n",
              "              0.0000e+00],\n",
              "            [ 0.0000e+00,  8.9407e-08, -8.9407e-08, -4.1723e-07, -8.9407e-08,\n",
              "              5.9605e-08],\n",
              "            [-4.7088e-06,  7.4506e-07,  0.0000e+00,  8.9407e-08, -3.3379e-06,\n",
              "             -2.0862e-07],\n",
              "            [ 5.9605e-08,  1.1921e-07,  2.9802e-08,  4.7684e-07, -8.9407e-08,\n",
              "              2.9802e-08]],\n",
              "\n",
              "           [[-2.9802e-08,  2.9802e-08,  2.9802e-08,  0.0000e+00,  0.0000e+00,\n",
              "              2.9802e-08],\n",
              "            [-4.1723e-07, -1.1921e-07,  2.9802e-08, -8.9407e-08, -2.6524e-06,\n",
              "              2.9802e-08],\n",
              "            [ 0.0000e+00, -2.9802e-08, -2.9802e-08, -2.9802e-08,  1.7881e-07,\n",
              "              1.1921e-07],\n",
              "            [-1.7881e-07,  1.1921e-07,  2.3842e-07, -2.6822e-07,  2.9802e-08,\n",
              "              3.5763e-07],\n",
              "            [ 8.9407e-08,  0.0000e+00,  5.9605e-08, -2.9802e-08,  5.6624e-07,\n",
              "              0.0000e+00]],\n",
              "\n",
              "           [[-2.6524e-06,  2.9802e-08,  8.9407e-08, -5.9605e-08, -5.9605e-08,\n",
              "              2.9802e-08],\n",
              "            [ 2.9802e-08, -5.9605e-08, -8.9407e-08, -1.7881e-07, -5.9605e-08,\n",
              "             -1.6987e-06],\n",
              "            [-8.9407e-08,  0.0000e+00, -1.7881e-07, -5.9605e-08, -8.9407e-08,\n",
              "             -5.9605e-08],\n",
              "            [-6.5565e-07, -4.4703e-07, -2.9802e-08, -1.1921e-07, -3.0696e-06,\n",
              "              2.3842e-07],\n",
              "            [-2.9802e-08,  5.9605e-08,  5.9605e-08, -2.0862e-07,  0.0000e+00,\n",
              "             -2.9802e-08]],\n",
              "\n",
              "           [[ 2.9802e-08,  0.0000e+00,  8.9407e-08, -8.9407e-08,  0.0000e+00,\n",
              "             -2.0862e-07],\n",
              "            [ 5.9605e-08,  2.0862e-07,  2.9802e-08, -2.9802e-08, -2.9802e-08,\n",
              "             -3.5763e-07],\n",
              "            [-2.9802e-08, -2.9802e-08,  3.8743e-07, -1.1921e-07, -3.5763e-07,\n",
              "              0.0000e+00],\n",
              "            [-1.4901e-07,  2.6822e-07, -1.1921e-07, -2.9802e-08, -1.7881e-07,\n",
              "              1.4901e-07],\n",
              "            [ 2.6822e-07, -5.9605e-08, -5.9605e-08,  2.3842e-07,  5.9605e-08,\n",
              "             -8.9407e-08]]],\n",
              "\n",
              "\n",
              "          [[[ 3.0994e-06,  2.9802e-08, -2.9802e-08,  3.5763e-07,  8.9407e-08,\n",
              "             -5.9605e-08],\n",
              "            [ 0.0000e+00, -8.9407e-08,  1.2219e-06, -2.9802e-08, -5.9605e-08,\n",
              "             -5.9605e-08],\n",
              "            [ 5.9605e-08, -6.5565e-07, -5.9605e-08,  5.9605e-08, -1.3113e-06,\n",
              "             -1.7881e-07],\n",
              "            [ 8.9407e-08, -2.9802e-08, -1.4901e-07, -2.9802e-08, -8.9407e-08,\n",
              "              1.7881e-07],\n",
              "            [ 4.4703e-07,  8.9407e-08, -2.9802e-08,  8.9407e-08,  2.9802e-08,\n",
              "             -2.9802e-07]],\n",
              "\n",
              "           [[ 0.0000e+00,  3.5763e-07,  5.9605e-08, -2.9802e-08, -3.2783e-07,\n",
              "             -1.1921e-07],\n",
              "            [ 2.9802e-08, -3.2783e-07,  1.1921e-07,  0.0000e+00,  2.9802e-08,\n",
              "             -1.1027e-06],\n",
              "            [-8.9407e-08,  4.2468e-07, -1.7881e-07,  2.9802e-08,  2.9802e-08,\n",
              "              2.6226e-06],\n",
              "            [ 2.9802e-08, -8.9407e-08, -2.9802e-08,  0.0000e+00,  1.4901e-07,\n",
              "             -5.9605e-08],\n",
              "            [-2.3842e-07, -1.4901e-07, -1.4901e-07, -8.9407e-08,  5.9605e-08,\n",
              "              0.0000e+00]],\n",
              "\n",
              "           [[-2.9802e-08,  0.0000e+00, -5.9605e-08,  1.1921e-07,  0.0000e+00,\n",
              "              2.9802e-08],\n",
              "            [ 5.9605e-08,  1.4901e-07,  5.9605e-08,  1.1921e-07,  0.0000e+00,\n",
              "             -2.0862e-07],\n",
              "            [ 8.9407e-08, -2.9802e-08,  0.0000e+00,  1.1325e-06, -2.9802e-08,\n",
              "              5.9605e-08],\n",
              "            [ 2.9802e-08,  2.3842e-07,  2.9802e-08,  2.9802e-08,  5.9605e-08,\n",
              "              3.2783e-07],\n",
              "            [ 2.9802e-08,  1.4901e-07, -5.9605e-08,  2.9802e-08,  4.7684e-07,\n",
              "             -2.9802e-08]],\n",
              "\n",
              "           [[-5.9605e-08,  8.9407e-08, -4.4703e-07,  0.0000e+00, -1.1921e-07,\n",
              "             -1.1921e-07],\n",
              "            [ 0.0000e+00, -8.9407e-08,  0.0000e+00, -3.5763e-07, -2.9802e-08,\n",
              "              3.5763e-07],\n",
              "            [ 2.9802e-08, -8.9407e-08,  1.7881e-07,  2.9802e-08,  2.9802e-08,\n",
              "              8.9407e-08],\n",
              "            [-2.0862e-07,  2.3842e-07,  0.0000e+00,  2.0117e-06,  2.3842e-07,\n",
              "              2.9802e-08],\n",
              "            [-4.1723e-07,  1.1921e-07, -5.9605e-08, -2.9802e-07, -1.6987e-06,\n",
              "              1.4603e-06]]],\n",
              "\n",
              "\n",
              "          [[[-3.5763e-07, -1.6689e-06, -3.5763e-07, -7.2122e-06, -5.9605e-08,\n",
              "             -2.9802e-08],\n",
              "            [ 0.0000e+00,  8.9407e-08, -2.9802e-08, -2.0862e-07,  2.9802e-08,\n",
              "              5.9605e-08],\n",
              "            [-2.9802e-08, -5.9605e-08, -4.1723e-07, -2.9802e-08,  4.1723e-07,\n",
              "              0.0000e+00],\n",
              "            [-8.9407e-08, -7.7486e-07, -1.1921e-07, -5.9605e-08,  0.0000e+00,\n",
              "              3.2783e-07],\n",
              "            [ 5.9605e-08, -1.7881e-07, -2.9802e-08,  2.9802e-08, -8.9407e-08,\n",
              "             -8.9407e-08]],\n",
              "\n",
              "           [[ 2.3842e-07, -1.4901e-07,  6.2585e-07,  5.9605e-08, -1.4901e-07,\n",
              "              1.1921e-07],\n",
              "            [ 8.9407e-08,  0.0000e+00,  1.1921e-07,  2.9802e-08,  2.9802e-08,\n",
              "             -1.1921e-07],\n",
              "            [-5.9605e-08,  1.1921e-07, -2.9802e-08, -5.9605e-08, -2.9802e-08,\n",
              "             -8.9407e-08],\n",
              "            [-5.9605e-08,  1.1921e-07, -9.2387e-07,  0.0000e+00, -8.9407e-08,\n",
              "              5.6624e-07],\n",
              "            [ 2.9802e-07,  0.0000e+00,  2.9802e-08,  8.9407e-08, -2.3842e-07,\n",
              "              1.1921e-07]],\n",
              "\n",
              "           [[ 3.2783e-06, -1.1921e-07, -1.1027e-06,  0.0000e+00, -2.9802e-08,\n",
              "              2.9802e-08],\n",
              "            [ 8.9407e-08, -2.9802e-08,  2.9802e-08,  5.9605e-08,  5.9605e-08,\n",
              "              0.0000e+00],\n",
              "            [-5.9605e-08,  2.0862e-07,  8.9407e-08, -5.9605e-08,  4.1723e-07,\n",
              "              0.0000e+00],\n",
              "            [ 2.9802e-08, -5.9605e-08, -2.9802e-08, -5.9605e-08,  0.0000e+00,\n",
              "              5.9605e-08],\n",
              "            [ 2.9802e-08, -6.8545e-07, -1.4901e-07, -8.9407e-08, -1.1921e-07,\n",
              "             -5.9605e-08]],\n",
              "\n",
              "           [[ 0.0000e+00,  5.9605e-08,  0.0000e+00,  2.9802e-07, -1.1921e-07,\n",
              "              5.9605e-08],\n",
              "            [ 5.9605e-08,  5.3644e-07, -8.9407e-08, -3.2783e-07, -2.9802e-08,\n",
              "              1.1921e-07],\n",
              "            [ 1.7881e-07,  3.5763e-07,  2.3842e-07,  2.9802e-08,  5.9605e-08,\n",
              "             -1.7881e-07],\n",
              "            [ 2.9802e-08,  1.7881e-07, -2.9802e-08, -5.9605e-08,  0.0000e+00,\n",
              "              2.9802e-08],\n",
              "            [ 1.7881e-07,  5.9605e-08, -2.9802e-08,  1.8477e-06,  2.0862e-07,\n",
              "             -8.9407e-08]]]],\n",
              "\n",
              "\n",
              "\n",
              "         [[[[-1.7881e-07,  8.9407e-08, -2.9802e-08, -4.4703e-07,  8.9407e-08,\n",
              "              2.9802e-08],\n",
              "            [ 2.3842e-07,  2.0862e-07, -2.0862e-07, -2.9802e-08, -2.9802e-08,\n",
              "              4.4703e-07],\n",
              "            [-8.9407e-08, -1.1921e-07,  2.6822e-07, -2.9802e-08, -5.6624e-07,\n",
              "             -4.8876e-06],\n",
              "            [-1.4901e-07,  5.9605e-08,  0.0000e+00,  5.9605e-08, -2.3842e-07,\n",
              "             -5.9605e-08],\n",
              "            [-1.1921e-07,  0.0000e+00,  2.9802e-08,  0.0000e+00,  5.9605e-08,\n",
              "             -5.9605e-08]],\n",
              "\n",
              "           [[ 2.9802e-07,  5.9605e-08,  8.9407e-08,  0.0000e+00,  8.9407e-08,\n",
              "              3.8743e-07],\n",
              "            [ 2.9802e-08,  5.9605e-08,  8.9407e-08,  2.9802e-08,  5.9605e-08,\n",
              "              1.1921e-07],\n",
              "            [-1.7881e-07,  0.0000e+00,  4.7684e-07,  5.9605e-08,  2.9802e-08,\n",
              "             -2.9802e-08],\n",
              "            [-1.8179e-06,  0.0000e+00,  2.9802e-08, -2.9802e-08, -5.9605e-08,\n",
              "              2.9802e-08],\n",
              "            [ 2.9802e-08,  1.4901e-07, -2.9802e-08,  0.0000e+00,  1.7881e-07,\n",
              "              5.9605e-08]],\n",
              "\n",
              "           [[-9.5367e-07, -5.9605e-08,  2.9802e-07,  8.0466e-07,  7.7486e-07,\n",
              "             -2.9802e-08],\n",
              "            [ 2.9802e-08, -9.5367e-07,  5.9605e-08,  2.9802e-08,  2.9802e-08,\n",
              "             -5.9605e-08],\n",
              "            [-5.9605e-08,  1.1623e-06, -2.9802e-08, -1.1921e-07, -2.9802e-07,\n",
              "              1.4305e-06],\n",
              "            [ 1.4901e-07,  5.9605e-08, -1.4901e-07, -8.9407e-08,  8.9407e-08,\n",
              "              8.9407e-08],\n",
              "            [-1.1921e-07,  2.9802e-08,  2.9802e-08, -1.4901e-07,  5.9605e-08,\n",
              "             -5.9605e-08]],\n",
              "\n",
              "           [[ 6.8545e-07,  2.9802e-08,  8.9407e-08, -2.9802e-08, -1.4901e-07,\n",
              "             -3.8743e-07],\n",
              "            [ 3.5763e-07,  6.9737e-06,  8.9407e-08, -4.1723e-07,  2.9802e-08,\n",
              "             -1.1921e-07],\n",
              "            [-2.9802e-08,  0.0000e+00, -5.9605e-08, -6.4820e-06, -8.9407e-08,\n",
              "              2.9802e-08],\n",
              "            [ 2.9802e-08,  0.0000e+00,  2.9802e-08,  2.9802e-08,  1.7881e-07,\n",
              "              2.9802e-08],\n",
              "            [-2.9802e-08,  1.3411e-06, -8.9407e-08,  2.9802e-08,  5.9605e-08,\n",
              "             -8.9407e-08]]],\n",
              "\n",
              "\n",
              "          [[[ 5.9605e-08,  2.9802e-08, -5.9605e-08, -9.2387e-07,  2.9802e-08,\n",
              "             -1.1921e-07],\n",
              "            [-2.9802e-08, -1.1921e-07,  1.6689e-06,  4.1723e-07, -2.9802e-08,\n",
              "              0.0000e+00],\n",
              "            [ 0.0000e+00,  5.9605e-08,  2.9802e-08, -5.9605e-08, -2.6822e-07,\n",
              "              2.9802e-08],\n",
              "            [ 0.0000e+00, -3.8147e-06,  2.0862e-07, -2.9802e-08,  2.0862e-07,\n",
              "              0.0000e+00],\n",
              "            [ 0.0000e+00,  0.0000e+00, -2.9802e-07,  2.0862e-07,  1.1921e-07,\n",
              "             -1.4901e-07]],\n",
              "\n",
              "           [[ 2.0862e-07,  7.1526e-07, -8.9407e-08,  0.0000e+00, -1.7881e-07,\n",
              "             -5.9605e-08],\n",
              "            [-5.0664e-07, -2.9802e-08,  1.7881e-07,  4.1723e-07,  4.1723e-07,\n",
              "              2.9802e-08],\n",
              "            [ 0.0000e+00,  1.4901e-07,  0.0000e+00,  5.9605e-08, -2.9802e-08,\n",
              "              1.1921e-07],\n",
              "            [ 1.1921e-07, -1.4901e-07, -8.9407e-08,  0.0000e+00,  5.9605e-08,\n",
              "             -5.9605e-08],\n",
              "            [-2.9802e-08,  1.1921e-07,  0.0000e+00,  2.9802e-08,  2.9802e-08,\n",
              "              5.9605e-08]],\n",
              "\n",
              "           [[-8.9407e-08, -2.3842e-07,  0.0000e+00,  8.9407e-08, -5.9605e-08,\n",
              "              2.9802e-08],\n",
              "            [-2.9802e-08,  2.9802e-08,  0.0000e+00, -1.1921e-07, -1.7881e-07,\n",
              "              0.0000e+00],\n",
              "            [-8.9407e-08, -2.9802e-08, -2.6822e-07,  2.0862e-07, -8.9407e-08,\n",
              "             -5.9605e-08],\n",
              "            [-2.9802e-08,  2.0862e-07,  2.9802e-08, -5.9605e-08,  2.9802e-08,\n",
              "             -2.9802e-08],\n",
              "            [ 5.9605e-08,  0.0000e+00,  2.9802e-08, -1.6093e-06,  1.1921e-07,\n",
              "             -3.2783e-06]],\n",
              "\n",
              "           [[ 0.0000e+00, -1.7881e-07, -1.4469e-05, -1.7881e-07, -5.3644e-07,\n",
              "              5.9605e-08],\n",
              "            [-1.4901e-07, -2.9802e-08,  5.9605e-08,  2.9802e-08,  5.9605e-08,\n",
              "              0.0000e+00],\n",
              "            [ 2.9802e-08, -4.1723e-07, -8.3447e-07,  2.0862e-07, -1.1921e-07,\n",
              "              0.0000e+00],\n",
              "            [ 1.1921e-07,  0.0000e+00, -5.9605e-08, -2.3842e-07,  5.9605e-08,\n",
              "              3.7253e-07],\n",
              "            [-5.9605e-08, -8.9407e-08, -2.9802e-08, -1.1921e-07, -3.5763e-07,\n",
              "              8.9407e-08]]],\n",
              "\n",
              "\n",
              "          [[[-8.9407e-08,  2.9802e-08, -4.4703e-07, -1.1921e-07, -1.1921e-07,\n",
              "             -7.7486e-07],\n",
              "            [-2.3842e-07,  5.9605e-08, -3.8743e-07,  0.0000e+00, -4.4703e-07,\n",
              "             -1.3113e-06],\n",
              "            [-1.1921e-07,  0.0000e+00,  5.9605e-08, -1.7881e-07,  1.4901e-07,\n",
              "             -2.3842e-07],\n",
              "            [ 2.9802e-08,  2.9802e-08,  1.1921e-07, -1.7881e-07,  2.9802e-08,\n",
              "              0.0000e+00],\n",
              "            [ 1.1921e-07,  2.9802e-08, -1.7881e-07,  2.9802e-08, -2.9802e-08,\n",
              "             -8.9407e-08]],\n",
              "\n",
              "           [[-1.1921e-07,  5.9605e-08,  8.3447e-07, -2.9802e-08,  6.8545e-07,\n",
              "             -2.9802e-07],\n",
              "            [-2.9802e-08,  2.0862e-07,  8.9407e-08, -2.9802e-08,  2.3842e-07,\n",
              "             -5.9605e-08],\n",
              "            [ 3.8743e-07, -1.1325e-06,  5.9605e-08,  4.1723e-07, -1.1921e-07,\n",
              "              5.9605e-08],\n",
              "            [ 1.4901e-07, -1.7881e-07,  2.9802e-08, -5.3644e-07, -1.0714e-05,\n",
              "              0.0000e+00],\n",
              "            [ 5.9605e-08,  0.0000e+00, -2.9802e-08, -2.9802e-07,  5.9605e-08,\n",
              "              0.0000e+00]],\n",
              "\n",
              "           [[-9.2387e-07, -2.9802e-08,  2.9802e-08,  1.1921e-07,  2.9802e-08,\n",
              "             -2.9802e-08],\n",
              "            [-2.9802e-08,  1.8775e-06, -2.9802e-08,  8.9407e-08,  0.0000e+00,\n",
              "              2.9802e-08],\n",
              "            [-2.9802e-08, -8.9407e-08,  1.7881e-07, -2.9802e-08,  8.9407e-08,\n",
              "             -1.2219e-06],\n",
              "            [ 0.0000e+00, -5.9605e-08, -1.4901e-07,  0.0000e+00,  8.9407e-08,\n",
              "             -2.9802e-08],\n",
              "            [ 5.9605e-08,  0.0000e+00,  1.7881e-07, -2.9802e-08,  3.2783e-07,\n",
              "              1.4901e-07]],\n",
              "\n",
              "           [[-1.4901e-07,  5.9605e-08,  0.0000e+00,  2.9802e-08,  0.0000e+00,\n",
              "             -2.0862e-07],\n",
              "            [ 2.9802e-08,  8.9407e-08,  0.0000e+00,  7.4506e-07, -8.9407e-08,\n",
              "              2.9802e-08],\n",
              "            [-3.5763e-07, -5.9605e-08,  0.0000e+00,  8.9407e-08,  5.9605e-08,\n",
              "             -8.9407e-08],\n",
              "            [ 2.6822e-07, -5.9605e-08,  1.1921e-07, -2.9802e-08,  0.0000e+00,\n",
              "              2.9802e-08],\n",
              "            [-5.9605e-08, -1.1921e-07, -2.9802e-08, -5.9605e-08, -2.9802e-08,\n",
              "              2.9802e-08]]]]],\n",
              "\n",
              "\n",
              "\n",
              "\n",
              "        [[[[[ 0.0000e+00, -1.3113e-06, -2.0862e-07, -5.9605e-08,  5.9605e-08,\n",
              "             -5.9605e-08],\n",
              "            [ 2.9802e-08, -4.7684e-07,  2.0862e-07,  6.6087e-06, -2.9802e-08,\n",
              "             -1.7881e-07],\n",
              "            [ 2.9802e-08, -2.9802e-08, -5.9605e-08,  5.9605e-08, -2.0862e-07,\n",
              "              8.9407e-08],\n",
              "            [ 2.9802e-08, -1.6689e-06, -8.9407e-08,  2.9802e-08,  2.9802e-08,\n",
              "             -2.9802e-08],\n",
              "            [-2.9802e-08,  0.0000e+00,  0.0000e+00,  5.9605e-08,  4.4703e-07,\n",
              "              0.0000e+00]],\n",
              "\n",
              "           [[-2.0862e-07, -2.9802e-08, -4.1723e-07, -8.9407e-08, -1.7881e-07,\n",
              "             -4.7684e-07],\n",
              "            [ 0.0000e+00, -1.1921e-07,  0.0000e+00, -3.5763e-07,  5.9605e-08,\n",
              "             -1.4901e-07],\n",
              "            [-1.4901e-07,  0.0000e+00, -8.9407e-08,  1.1921e-07,  2.3842e-07,\n",
              "             -1.4901e-07],\n",
              "            [ 1.9968e-06, -2.9802e-08,  2.9802e-08,  1.1921e-07,  2.9802e-08,\n",
              "             -2.5509e-06],\n",
              "            [ 2.9802e-08, -1.3858e-05, -2.3842e-07, -5.9605e-08, -2.0862e-07,\n",
              "              5.9605e-08]],\n",
              "\n",
              "           [[ 2.0862e-07, -8.9407e-08, -1.1921e-07, -2.9802e-08,  0.0000e+00,\n",
              "              1.1921e-07],\n",
              "            [ 5.9605e-08,  2.3842e-07,  2.9802e-08, -2.9802e-07, -1.1325e-06,\n",
              "             -1.4901e-07],\n",
              "            [ 1.7881e-07,  5.9605e-08, -1.4901e-07, -4.1723e-07,  2.9802e-08,\n",
              "              2.0862e-07],\n",
              "            [-1.1921e-07,  0.0000e+00, -5.9605e-08, -2.9802e-08, -8.9407e-08,\n",
              "              3.5763e-07],\n",
              "            [ 1.1921e-07, -1.1921e-07,  2.9802e-08,  2.9802e-08,  0.0000e+00,\n",
              "              2.9802e-08]],\n",
              "\n",
              "           [[ 9.5367e-07,  8.9407e-08,  0.0000e+00, -2.0862e-07, -8.9407e-08,\n",
              "             -4.1723e-07],\n",
              "            [ 5.9605e-08,  0.0000e+00, -2.0862e-07,  8.9407e-08,  2.9802e-08,\n",
              "              6.8545e-07],\n",
              "            [-1.7881e-07,  5.9605e-08, -2.9802e-08,  3.5763e-06,  0.0000e+00,\n",
              "              5.9605e-08],\n",
              "            [-8.9407e-08,  0.0000e+00, -5.9605e-08,  5.9605e-08,  2.9802e-08,\n",
              "             -8.9407e-08],\n",
              "            [-1.4901e-07, -2.9802e-08,  0.0000e+00,  1.4901e-07, -5.9605e-08,\n",
              "              2.9802e-08]]],\n",
              "\n",
              "\n",
              "          [[[ 6.8545e-07,  4.4703e-07, -2.0862e-07,  1.1921e-07,  5.9605e-08,\n",
              "              2.9802e-08],\n",
              "            [ 0.0000e+00, -1.1921e-07, -5.9605e-08,  2.9802e-08,  5.9605e-08,\n",
              "              0.0000e+00],\n",
              "            [-2.9802e-08,  0.0000e+00, -4.1723e-07,  2.9802e-08, -1.4901e-07,\n",
              "              0.0000e+00],\n",
              "            [-1.7881e-07,  0.0000e+00,  1.4603e-06,  1.4901e-07,  2.9802e-08,\n",
              "             -1.7881e-07],\n",
              "            [ 0.0000e+00, -2.9802e-08,  0.0000e+00,  5.9605e-08, -2.9802e-08,\n",
              "              3.8743e-07]],\n",
              "\n",
              "           [[-7.7486e-07, -5.9605e-08,  7.7486e-07,  0.0000e+00, -3.8743e-07,\n",
              "             -1.7881e-07],\n",
              "            [ 8.9407e-08, -3.9935e-06,  2.6822e-07, -1.5199e-06,  1.1921e-07,\n",
              "              5.9605e-08],\n",
              "            [ 2.9802e-08, -1.7881e-07,  2.9802e-08,  5.9605e-08, -2.9802e-08,\n",
              "              2.3842e-07],\n",
              "            [-1.4901e-07,  1.7881e-07,  2.9802e-08,  5.9605e-08, -2.9802e-08,\n",
              "              5.9605e-08],\n",
              "            [ 1.7881e-07, -8.9407e-08, -6.2585e-07,  0.0000e+00,  2.0862e-07,\n",
              "              2.9802e-08]],\n",
              "\n",
              "           [[-2.9802e-08, -2.0862e-07,  2.3842e-07, -5.6624e-07,  2.9802e-08,\n",
              "             -5.9605e-08],\n",
              "            [-2.9802e-08,  0.0000e+00,  2.9802e-08, -8.9407e-08, -2.9802e-08,\n",
              "              5.9605e-08],\n",
              "            [ 2.9802e-08,  2.0862e-07,  5.9605e-08,  5.9605e-08, -2.2352e-06,\n",
              "              0.0000e+00],\n",
              "            [ 2.9802e-08, -1.1921e-06,  5.9605e-08, -2.9802e-07, -3.2783e-07,\n",
              "              2.9802e-08],\n",
              "            [-5.9605e-08,  2.9802e-08,  0.0000e+00, -5.9605e-08, -2.9802e-08,\n",
              "             -2.9802e-08]],\n",
              "\n",
              "           [[-2.9802e-08,  5.9605e-08, -2.9802e-08,  5.9605e-08,  3.9637e-06,\n",
              "              5.9605e-08],\n",
              "            [ 2.9802e-08, -5.9605e-08,  4.1723e-07,  2.9802e-08, -8.9407e-08,\n",
              "              5.9605e-08],\n",
              "            [ 1.1921e-07, -5.9605e-08, -3.8743e-07, -2.0862e-07, -2.9802e-08,\n",
              "             -2.3842e-07],\n",
              "            [-5.5432e-06,  5.9605e-08,  2.6822e-07,  4.4703e-07,  2.0862e-07,\n",
              "              2.9802e-08],\n",
              "            [ 2.9802e-08,  0.0000e+00,  5.9605e-08,  2.9802e-08, -5.9605e-08,\n",
              "              2.9802e-08]]],\n",
              "\n",
              "\n",
              "          [[[-1.1921e-07,  1.5944e-06, -5.9605e-08,  2.9802e-08, -5.9605e-08,\n",
              "             -2.0862e-07],\n",
              "            [-8.9407e-08,  1.6689e-06,  1.4901e-07,  5.9605e-08,  3.2783e-07,\n",
              "              1.8356e-06],\n",
              "            [ 5.9605e-08, -1.4901e-07,  5.9605e-08,  8.9407e-08,  5.9605e-08,\n",
              "              2.9802e-08],\n",
              "            [-5.9605e-08, -5.9605e-08, -8.9407e-08,  0.0000e+00, -5.9605e-08,\n",
              "              2.3842e-07],\n",
              "            [ 2.9802e-08, -1.7583e-06,  0.0000e+00, -5.9605e-08,  2.6822e-07,\n",
              "              2.0862e-07]],\n",
              "\n",
              "           [[-1.7881e-07,  0.0000e+00, -1.1921e-07, -1.1921e-07, -2.9802e-07,\n",
              "              2.9802e-08],\n",
              "            [ 5.9605e-08, -2.9802e-08,  0.0000e+00,  2.9802e-08, -2.9802e-08,\n",
              "             -3.2783e-07],\n",
              "            [ 8.9407e-08,  2.9802e-08, -3.2783e-07, -8.9407e-08,  1.7881e-07,\n",
              "             -5.9605e-08],\n",
              "            [ 0.0000e+00,  2.9802e-08, -2.9802e-08, -2.9802e-08,  2.0862e-07,\n",
              "              2.9802e-08],\n",
              "            [-2.3842e-07, -5.9605e-08, -2.9802e-08,  1.4901e-07,  2.9802e-08,\n",
              "             -1.4901e-07]],\n",
              "\n",
              "           [[-8.9407e-08, -5.9605e-08, -4.4703e-07, -1.7881e-07,  1.1921e-07,\n",
              "              2.9802e-08],\n",
              "            [ 8.9407e-08, -1.2517e-06, -1.7881e-06, -5.9605e-08,  0.0000e+00,\n",
              "              2.9802e-08],\n",
              "            [-5.9605e-08,  5.9605e-08,  2.9802e-07, -5.9605e-08, -2.9802e-07,\n",
              "             -2.0862e-07],\n",
              "            [-2.9802e-08,  0.0000e+00,  2.9802e-08, -1.1921e-07, -1.4901e-07,\n",
              "              2.6822e-07],\n",
              "            [-5.9605e-08,  1.1921e-07,  1.1921e-07, -5.9605e-08, -2.9802e-08,\n",
              "             -3.8743e-07]],\n",
              "\n",
              "           [[ 1.1921e-07,  5.9605e-08,  5.9605e-08,  1.4901e-07, -1.1921e-07,\n",
              "              2.9802e-08],\n",
              "            [ 1.1921e-07,  8.9407e-08, -8.9407e-08,  1.4901e-07,  2.2650e-06,\n",
              "              0.0000e+00],\n",
              "            [ 0.0000e+00, -7.1526e-07, -1.4901e-07, -8.9407e-08,  2.9802e-08,\n",
              "              0.0000e+00],\n",
              "            [-1.5795e-06,  8.9407e-08,  5.9605e-08,  0.0000e+00, -8.9407e-08,\n",
              "              5.9605e-08],\n",
              "            [-3.9339e-06, -8.9407e-08,  4.1723e-07, -8.9407e-08, -2.9802e-08,\n",
              "              0.0000e+00]]]],\n",
              "\n",
              "\n",
              "\n",
              "         [[[[-2.9802e-08, -4.1723e-07,  2.9802e-08, -8.9407e-08, -1.4901e-07,\n",
              "              2.9802e-08],\n",
              "            [ 2.9802e-08,  2.9802e-07,  2.9802e-08,  5.9605e-08, -2.6822e-07,\n",
              "              8.9407e-08],\n",
              "            [-2.9802e-08,  1.4901e-07, -2.9802e-08,  6.5565e-07,  8.9407e-08,\n",
              "              0.0000e+00],\n",
              "            [ 0.0000e+00,  0.0000e+00, -2.9802e-08,  2.9802e-08,  1.7881e-06,\n",
              "              1.4901e-07],\n",
              "            [-2.9802e-08, -2.9802e-08,  2.9802e-08,  2.9802e-08,  4.1723e-07,\n",
              "             -4.7386e-06]],\n",
              "\n",
              "           [[-8.9407e-08, -2.9802e-08, -5.9605e-08, -1.1921e-07,  2.0862e-07,\n",
              "             -8.9407e-08],\n",
              "            [ 5.9605e-08, -2.9802e-07, -2.9802e-08, -8.9407e-08,  2.9802e-08,\n",
              "             -2.9802e-08],\n",
              "            [-8.9407e-08,  2.6822e-07, -5.9605e-08,  2.9802e-08, -5.9605e-08,\n",
              "              1.4901e-07],\n",
              "            [ 5.9605e-08,  8.9407e-08,  0.0000e+00, -9.2387e-07, -5.9605e-08,\n",
              "              0.0000e+00],\n",
              "            [ 1.4901e-07,  2.0862e-07,  0.0000e+00,  5.9605e-08,  0.0000e+00,\n",
              "              2.9802e-08]],\n",
              "\n",
              "           [[-1.3113e-06, -1.7881e-07,  2.9802e-08, -1.1921e-07,  2.9802e-08,\n",
              "             -2.9802e-08],\n",
              "            [-2.9802e-08,  4.4703e-07,  0.0000e+00,  5.9605e-08, -2.9802e-08,\n",
              "             -5.9605e-08],\n",
              "            [-8.9407e-08,  8.9407e-08, -1.7881e-07, -1.1921e-07, -8.0466e-07,\n",
              "              2.0862e-07],\n",
              "            [ 5.9605e-08,  5.9605e-08,  8.9407e-08, -8.9407e-08,  0.0000e+00,\n",
              "             -8.9407e-08],\n",
              "            [ 2.0862e-07,  2.3842e-07, -3.5763e-07,  2.6822e-07,  2.9802e-08,\n",
              "              5.9605e-08]],\n",
              "\n",
              "           [[ 1.4901e-07,  3.8743e-07,  0.0000e+00, -4.4703e-07, -1.1921e-07,\n",
              "              8.9407e-08],\n",
              "            [-7.4506e-07,  3.2783e-07, -2.9802e-08, -5.9605e-08,  2.9802e-08,\n",
              "              5.9605e-08],\n",
              "            [ 2.6822e-07, -8.9407e-08,  2.9802e-08,  0.0000e+00, -2.9802e-08,\n",
              "              5.9605e-08],\n",
              "            [-1.4901e-07, -2.9802e-08, -2.9802e-08,  2.9802e-08,  5.9605e-08,\n",
              "              1.2815e-06],\n",
              "            [-2.9802e-08,  0.0000e+00,  1.7881e-07, -5.9605e-08,  5.9605e-08,\n",
              "              4.7684e-07]]],\n",
              "\n",
              "\n",
              "          [[[ 4.7684e-07, -2.9802e-08,  5.9605e-08, -8.9407e-08,  1.1921e-07,\n",
              "              5.9605e-08],\n",
              "            [ 2.9802e-08,  0.0000e+00, -1.1921e-07, -5.9605e-08, -8.9407e-08,\n",
              "             -8.9407e-08],\n",
              "            [-3.1292e-06,  5.9605e-08,  2.9802e-08,  2.9802e-08, -2.9802e-08,\n",
              "              2.9802e-08],\n",
              "            [-5.9605e-08,  0.0000e+00,  8.9407e-07,  8.9407e-08, -2.6524e-06,\n",
              "             -3.1292e-06],\n",
              "            [ 5.9605e-08,  5.9605e-08,  2.9802e-08, -5.9605e-07, -5.9605e-08,\n",
              "             -8.6427e-07]],\n",
              "\n",
              "           [[ 0.0000e+00,  1.1921e-07,  1.1921e-07, -3.8743e-07, -2.6822e-07,\n",
              "              2.2054e-06],\n",
              "            [ 2.9802e-08, -1.4901e-07, -2.9802e-08,  4.3362e-06, -2.9802e-08,\n",
              "              1.6987e-06],\n",
              "            [ 2.9802e-08, -2.9802e-08,  2.3842e-07, -5.9605e-08, -2.9802e-08,\n",
              "             -2.9802e-08],\n",
              "            [ 7.4506e-07,  1.7881e-07, -2.9802e-08,  1.3411e-06,  1.4901e-07,\n",
              "              2.3842e-07],\n",
              "            [ 1.1921e-07, -1.4901e-07, -4.4703e-07,  2.9802e-08,  0.0000e+00,\n",
              "             -9.8348e-07]],\n",
              "\n",
              "           [[-1.4901e-06,  1.7881e-07,  2.9802e-08,  2.0862e-07, -8.9407e-07,\n",
              "              0.0000e+00],\n",
              "            [-1.8179e-06, -2.9802e-08, -2.9802e-08,  2.3842e-07,  0.0000e+00,\n",
              "              0.0000e+00],\n",
              "            [ 9.8348e-07, -1.7881e-07,  0.0000e+00, -5.9605e-08, -7.1526e-07,\n",
              "             -8.9407e-08],\n",
              "            [ 0.0000e+00, -5.9605e-08, -2.9802e-08,  5.9605e-08, -1.8477e-06,\n",
              "             -2.9802e-08],\n",
              "            [-5.9605e-08, -9.8944e-06, -1.3128e-05,  2.9802e-08,  5.9605e-08,\n",
              "             -2.0862e-07]],\n",
              "\n",
              "           [[ 8.9407e-08, -5.9605e-08,  2.0862e-07,  2.9802e-08,  3.5763e-06,\n",
              "              2.0862e-07],\n",
              "            [ 4.1723e-07, -8.0466e-07,  5.9605e-08,  8.6427e-07,  8.9407e-08,\n",
              "             -5.9605e-08],\n",
              "            [-1.7881e-06, -5.9605e-08, -2.0862e-07,  0.0000e+00,  1.4901e-07,\n",
              "              5.9605e-08],\n",
              "            [-2.0862e-07, -2.3842e-07, -2.9802e-07, -2.0862e-07,  2.3842e-07,\n",
              "             -1.1921e-07],\n",
              "            [-2.9802e-08, -1.1921e-07, -5.9605e-08, -8.9407e-08, -1.4901e-07,\n",
              "              0.0000e+00]]],\n",
              "\n",
              "\n",
              "          [[[-1.2517e-06,  0.0000e+00, -8.9407e-08,  2.9802e-08, -2.3842e-07,\n",
              "              0.0000e+00],\n",
              "            [ 5.9605e-08,  1.1921e-07,  2.9802e-08, -4.9472e-06, -1.1027e-06,\n",
              "             -2.9802e-08],\n",
              "            [-2.0862e-07,  3.8743e-07, -1.7881e-07, -2.9802e-08, -8.9407e-08,\n",
              "              5.9605e-08],\n",
              "            [ 8.9407e-08, -5.9605e-08,  5.9605e-08,  2.9802e-08,  0.0000e+00,\n",
              "             -8.9407e-08],\n",
              "            [-3.7551e-06, -1.4901e-07,  2.0862e-07, -1.4901e-07, -2.9802e-08,\n",
              "             -8.9407e-08]],\n",
              "\n",
              "           [[ 2.0862e-07,  2.9802e-08, -8.9407e-08,  5.9605e-08, -2.9802e-08,\n",
              "             -7.7486e-07],\n",
              "            [ 6.2585e-07, -2.9802e-08, -2.9802e-08, -2.0862e-07,  5.9605e-08,\n",
              "              0.0000e+00],\n",
              "            [ 2.9802e-08,  3.9041e-06, -2.9802e-08,  1.1921e-07,  8.9407e-08,\n",
              "              0.0000e+00],\n",
              "            [ 2.9802e-07,  1.4901e-07,  2.9802e-08, -2.9802e-08, -2.9802e-08,\n",
              "              0.0000e+00],\n",
              "            [ 1.2517e-06,  0.0000e+00,  2.9802e-08, -8.3447e-07, -2.3842e-07,\n",
              "             -8.9407e-08]],\n",
              "\n",
              "           [[ 2.9802e-08,  0.0000e+00,  1.7881e-07,  2.3842e-07,  5.9605e-08,\n",
              "             -2.9802e-08],\n",
              "            [ 5.9605e-08,  2.9802e-08, -2.9802e-08, -8.9407e-08, -2.9802e-07,\n",
              "             -1.5199e-06],\n",
              "            [ 1.1921e-07, -2.3842e-07, -5.9605e-08,  1.6689e-06, -1.7881e-07,\n",
              "              2.9802e-08],\n",
              "            [ 4.1723e-07,  0.0000e+00, -2.0862e-07, -2.3544e-06,  2.0862e-07,\n",
              "              2.9802e-08],\n",
              "            [-2.9802e-08, -2.9802e-08,  4.7684e-07, -2.9802e-08,  9.2387e-07,\n",
              "              0.0000e+00]],\n",
              "\n",
              "           [[-3.8743e-07,  1.1921e-07,  0.0000e+00,  2.6822e-07,  8.9407e-08,\n",
              "             -1.7881e-07],\n",
              "            [ 7.7486e-07,  2.9802e-08, -5.9605e-07, -5.9605e-07,  1.1921e-07,\n",
              "             -5.9605e-08],\n",
              "            [ 5.9605e-08, -1.7881e-07,  0.0000e+00, -1.7881e-07,  2.3842e-06,\n",
              "             -8.9407e-08],\n",
              "            [-2.9802e-08, -1.1921e-07,  2.9802e-08, -5.9605e-08,  2.9802e-08,\n",
              "              0.0000e+00],\n",
              "            [ 2.3842e-07,  2.9802e-08, -6.5565e-07,  3.2783e-07, -1.4901e-07,\n",
              "              3.2783e-07]]]]]], grad_fn=<MeanBackward1>)"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "pNCBGNWpZk-q"
      },
      "execution_count": 32,
      "outputs": []
    }
  ]
}